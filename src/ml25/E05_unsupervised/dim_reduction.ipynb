{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "En este ejercicio exploraremos el uso de PCA y t-SNE usando las librerias de scikit-learn. \n",
    "Específicamente, reduciremos la dimensionalidad de los datos para analizar cuando un tumor es benigno o maligno.\n",
    "\n",
    "Para entender el uso de estos metodos con scikit-learn, puedes consultar la documentación oficial de scikit-learn para [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "En este ejercicio:\n",
    "\n",
    "1. Utilizarás y explorarás un dataset de clasificación de tumores\n",
    "2. Utilizarás PCA para poder visualizar tus datos de 30 variables en 2 dimensiones\n",
    "3. Analizarás la proyección de tus datos a una dimensionalidad más baja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explorando tus datos\n",
    "En las siguientes celdas, cargamos el dataset que utilizamos en el ejercicio de regresión logística. Este es un conjunto de datos de clasificación, donde cada punto tiene 30 atributos o variables de entrada. Corre la siguiente celda para cargar el dataset y separar los datos en entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos originales de dimensionalidad (426, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# En esta ocasión leeremos el dataset como un dataframe de pandas\n",
    "dataset = load_breast_cancer(as_frame=True)\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    dataset['data'], \n",
    "    dataset['target'], \n",
    "    test_size=0.25,\n",
    "    random_state=10\n",
    ")\n",
    "print(f\"Datos originales de dimensionalidad {data_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos imprimir los primeros tres renglones de nuestros datos. Esto nos sevirá para identificar como son los datos originales antes de aplicar la reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteristicas\n",
      "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "327        12.03         17.93           76.09      446.0          0.07683   \n",
      "60         10.17         14.88           64.55      311.9          0.11340   \n",
      "260        20.31         27.06          132.90     1288.0          0.10000   \n",
      "\n",
      "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "327           0.03892        0.001546             0.005592         0.1382   \n",
      "60            0.08061        0.010840             0.012900         0.2743   \n",
      "260           0.10880        0.151900             0.093330         0.1814   \n",
      "\n",
      "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
      "327                 0.06070  ...         13.07          22.25   \n",
      "60                  0.06960  ...         11.02          17.45   \n",
      "260                 0.05572  ...         24.33          39.16   \n",
      "\n",
      "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
      "327            82.74       523.4            0.1013            0.07390   \n",
      "60             69.86       368.6            0.1275            0.09866   \n",
      "260           162.30      1844.0            0.1522            0.29450   \n",
      "\n",
      "     worst concavity  worst concave points  worst symmetry  \\\n",
      "327         0.007732               0.02796          0.2171   \n",
      "60          0.021680               0.02579          0.3557   \n",
      "260         0.378800               0.16970          0.3151   \n",
      "\n",
      "     worst fractal dimension  \n",
      "327                  0.07037  \n",
      "60                   0.08020  \n",
      "260                  0.07999  \n",
      "\n",
      "[3 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar las primeras tres filas\n",
    "print(\"Caracteristicas\")\n",
    "print(data_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas\n",
      "327    1\n",
      "60     1\n",
      "260    0\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Etiquetas\")\n",
    "print(target_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementación de PCA\n",
    "En clase vimos que la solución de PCA está dada por los vectores propios de la matriz de covarianza. Ya que estós determinan la mejor dirección de proyección según las asunciones del método. Para que esto sea cierto, espera que los datos usados para calcular la matriz de covarianza se encuentren normalizados. La función para normalizar la distribución se encuentra implementada de la siguiente manera:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 2)\n"
     ]
    }
   ],
   "source": [
    "def norm_data(X):\n",
    "    mean = X.mean(0)\n",
    "    std = X.std(0)\n",
    "    X_proc = (X - mean)\n",
    "    new_data = X_proc / std\n",
    "    return new_data\n",
    "\n",
    "feat1, feat2 = \"mean radius\", \"mean compactness\"\n",
    "X = data_train[[feat1, feat2]].to_numpy()\n",
    "X = norm_data(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, termina el método para calcular los valores y vectores propios de la matríz de covarianza.\n",
    "Recuerda que los vectores propios $v \\in \\mathbb{R}^{D \\times D}$ donde `v_[:, i]` corresponde al vector (columna) propio $v_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.70710678, -0.70710678],\n",
       "        [ 0.70710678,  0.70710678]]),\n",
       " array([1.520137  , 0.48456888]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_principal_components(X):\n",
    "    '''\n",
    "        Calcula los vectores y valores propios de una matriz con media cero X\n",
    "        args:\n",
    "            - X  (np.ndarray): Matriz de datos de dimensionalidad N x D\n",
    "        returns:\n",
    "            - eigenvectors (np.ndarray): vectores propios de la matriz de cov. de X.  Dimensionalidad de D x D\n",
    "            - eigenvalues (np.ndarray, dtype=float): valores propios de la matriz de cov. de X en el mismo orden que los vectores propios\n",
    "    '''\n",
    "    x_normalizada = norm_data(X)\n",
    "    sample_cov = np.cov(x_normalizada, rowvar=0)\n",
    "    # TODO: Calcula los vectores propios y regresalos en orden \n",
    "    # del mayor al menor valor propio. \n",
    "    # Investiga:\n",
    "    #   - la función np.argsort\n",
    "    #   - Como invertir una lista en python\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(sample_cov)\n",
    "    indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[indices]\n",
    "    eigenvectors = eigenvectors[:,indices]\n",
    "    return eigenvectors, eigenvalues\n",
    "get_principal_components(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez implementado el método anterior, corre la siguiente celda para visualizar los vectores propios sobre los datos normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 2)\n",
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mean compactness')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat1, feat2 = \"mean radius\", \"mean compactness\"\n",
    "labels = target_train\n",
    "X = data_train[[feat1, feat2]].to_numpy()\n",
    "X = norm_data(X)\n",
    "print(X.shape)\n",
    "eigvecs, eigvals = get_principal_components(X)\n",
    "print(eigvecs.shape)\n",
    "# ========== Graficas ===============\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "# Espacio original\n",
    "for c in range(2):\n",
    "    ax.scatter(\n",
    "        X[:, 0][labels == c], \n",
    "        X[:, 1][labels == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "c = ['r','b']\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = np.mean(X, axis=0)\n",
    "    end_point = eigvecs[:, idx_col] * norm_eig\n",
    "    color = c[idx_col]\n",
    "    ax.arrow(*start_point, *end_point, facecolor=color, width = 0.03, head_width = 0.3)\n",
    "ax.axis('equal')\n",
    "ax.set_title('Datos con media cero')\n",
    "ax.set_xlabel(feat1)\n",
    "ax.set_ylabel(feat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responde:\n",
    "- Si proyectamos los puntos a ambos vectores propios ¿cual nos dará una mejor separabilidad de clase (azul, rojo)?\n",
    "- Usando la imagen anterior como referencia ¿Cual de los dos vectores propios tiene un mayor valor propio (azul, rojo)? Explica la motivación de tu respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Proyección a una dimensión\n",
    "Ahora que has visto tus datos considerando solo dos variables distintas, vamos a proyectar los datos al primer componente principal ambos para visualizar como se verían en 1 sola dimensión.\n",
    "Completa y corre las siguientes celdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_data_train = norm_data(data_train)\n",
    "# Datos normalizados (media cero y misma escala)\n",
    "X = normed_data_train[[feat1, feat2]].to_numpy()\n",
    "eigvecs, eigvals = get_principal_components(X)\n",
    "print(eigvecs, eigvals)\n",
    "n = X.shape[0]\n",
    "\n",
    "# TODO: Proyecta los datos al primer componente principal eigvecs[:,0]\n",
    "X_proj1 = ...\n",
    "\n",
    "# TODO: Proyecta los datos al segundo componente principal eigvecs[:,1]\n",
    "X_proj2 = ...\n",
    "\n",
    "assert (X_proj1.shape == X_proj2.shape and X_proj1.squeeze().shape == (n,)), \"Las proyecciones no tienen la dimensionalidad correcta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Solución de scikit learn ============#\n",
    "from sklearn.decomposition import PCA\n",
    "# TODO: Utiliza la libreria de scikit-learn para reducir X a 1 dimensión\n",
    "# Invesitga PCA en scikit learn\n",
    "reduced_x = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, utilizamos tu implementación de pca para graficar la proyección a los componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuerda siempre correr la celda anterior antes de correr esta\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15, 3))\n",
    "# Espacio original\n",
    "for c in range(2):\n",
    "    ax1.scatter(\n",
    "        X[:, 0][labels == c], \n",
    "        X[:, 1][labels == c], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "\n",
    "# Graficamos los vectores sobrepuestos a los datos en el ax anterior\n",
    "for idx_col in range(eigvals.shape[0]):\n",
    "    norm_eig = np.sqrt(eigvals[idx_col])\n",
    "    start_point = np.mean(X, axis=0)\n",
    "    end_point = eigvecs[:, idx_col] * norm_eig\n",
    "    ax1.arrow(*start_point, *end_point, width = 0.03, head_width = 0.3)\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('Datos con media cero')\n",
    "ax1.set_xlabel(feat1)\n",
    "ax1.set_ylabel(feat2)\n",
    "\n",
    "for c in range(2):\n",
    "    class_data = X_proj1[labels == c]\n",
    "    N_samples = len(class_data)\n",
    "    ax2.scatter(\n",
    "        class_data, \n",
    "        np.zeros(N_samples), \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax2.set_title('Proyección al 1er CP')\n",
    "\n",
    "for c in range(2):\n",
    "    class_data = X_proj2[labels == c]\n",
    "    N_samples = len(class_data)\n",
    "    ax3.scatter(\n",
    "        class_data, \n",
    "        np.zeros(N_samples), \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax3.set_title('Proyección al 2do CP')\n",
    "\n",
    "\n",
    "## Solución de scikit-learn (1 dim)\n",
    "# plot reduced data\n",
    "for c in range(2):\n",
    "    class_idx = np.where(target_train == c)[0]\n",
    "    N_samples = len(class_idx)\n",
    "    ax4.scatter(\n",
    "        reduced_x[class_idx][:,0],\n",
    "        np.zeros(N_samples),\n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax4.set_title('Solución de scikit-learn (1 dimensión)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la tarea de clasificación responde:\n",
    "- ¿Cual vector propio crees que te permita separar mejor los datos al proyectarse en el? Explica tu intuición.\n",
    "- ¿Se cumple en este caso la asunción de PCA (la importancia de una dimensión está determinada por la varianza de la proyección a la misma)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Usando PCA para reducir 30 dimensiones a 2\n",
    "\n",
    "En la sección anterior, filtramos nuestros datos originales para quedarnos solo con dos variables. En esta sección consideramos los datos en su estado original, es decir con las 30 variables de entrada. Por su puesto, es imposible visualizarlos de esta forma, por lo tanto usaremos PCA para reducir la dimensionalidad de los datos originales a solo 2 dimensiones para poder visualizarlos.\n",
    "\n",
    "Usando PCA podemos transformar un dataset de $D$ variables proyectándolos a los $K$ vectores propios con los mayores valores propios perdiendo la menor información posible. De esta manera PCA es una función tal que:\n",
    "\n",
    "$X_{orig} \\in \\mathbb{R}^{N \\times D} \\mapsto X_{reduced} \\in \\mathbb{R}^{N \\times 2}$\n",
    "\n",
    "Utiliza la implementación de sklearn para realizar la proyección y graficar los datos proyectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = data_train.to_numpy()\n",
    "print(X.shape)\n",
    "output_dims = 2\n",
    "\n",
    "# TODO: Reduce la dimensionalidad de X a --> (N, 2) usando la implementación de PCA de sklearn\n",
    "# No es necesario normalizar los datos\n",
    "#========= START =============#\n",
    "X_low_dim = ...\n",
    "#========== END ==============#\n",
    "\n",
    "assert (X_low_dim.shape[-1] == output_dims), f\"Los datos se estan reduciendo de {X.shape} a {X_low_dim.shape} pero debería ser ({X.shape[0]}, {output_dims})\"\n",
    "print(f\"Scikit learn: Datos originales {X.shape}, Datos proyectados {X_low_dim.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO grafica los datos de dimensionalidad reducida en 2 dimensiones\n",
    "# investiga plt.scatter y np.where\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "for clase in range(2):\n",
    "    class_idx = np.where(target_train == clase)\n",
    "    ax.scatter(\n",
    "        X_low_dim[class_idx, 0],\n",
    "        X_low_dim[class_idx, 1],\n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax.set_title(\"La solución de scikit-learn\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros métodos de reducción de dimensionalidad\n",
    "\n",
    "Como PCA, existen otras alternativas para reducir la dimensionalidad de los datos. Una muy popular es el algoritmo de TSNE.\n",
    "Mientras que ambos métodos sirven el mismo propósito, se utilizan en situaciones distintas. PCA normalmente se utiliza cuando queremos reducir dimensionalidades muy altas, a unas pequeñas por ejemplo reducir de 100 a 50 o 10 dimensiones. \n",
    "\n",
    "Por otro lado TSNE funciona bien cuando tenemos una dimensionalidad relativamente baja (entre 20 y 5 por ejemplo) a una más pequeña que podamos visualizar (por ejemplo entre 3 y 1).\n",
    "TSNE tiene un hiperparámetro adicional a la cantidad de componentes de salida, llamado \"perplexidad\". Si te interesa conocer más sobre TSNE puedes revisar la [documentación de sci-kit learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). TSNE se utiliza principalmente para análisis y visualización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X = data_train.to_numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=50)\n",
    "reduced_x = tsne.fit_transform(X)\n",
    "print(reduced_x.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "for c in range(2):\n",
    "    class_idx = np.where(target_train == c)\n",
    "    ax.scatter(\n",
    "        reduced_x[class_idx, 0],\n",
    "        reduced_x[class_idx,1], \n",
    "        label = f\"class {c}\"\n",
    "    )\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reducción de dimensionalidad aplicado a tareas adicionales\n",
    "PCA normalmente se utiliza cuando queremos visualizar lo que ha aprendido nuestro algoritmo para entradas de alta dimensionalidad. Tenemos dos opciones\n",
    "\n",
    "1. Podemos entrenar con el dataset reducido un algoritmo de clasificación, como regresión logística.\n",
    "2. Podemos entrenar regresión logística en alta dimensionalidad y usar PCA para reducir la dimensionalidad y visualizar el resultado.\n",
    "\n",
    "En ambos casos necesitamos poder reducir la dimensionalidad. En esta sección, utiliza la librería de sklearn para resolver lo que se te indique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Entrenando en alta dimensionalidad\n",
    "En las siguientes celdas probaremos analizar la calidad de la predicción si entrenamos en alta y en baja dimensionalidad. Completa la siguiente celda  para generar las predicciones del conjunto de entrenamiento al entrenar en alta y baja dimensionalidad respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ====================  ALTA dimensionalidad ====================#\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def train_pred_high_dim(x_train, y_train, x_test, y_test, dim_red='PCA'):\n",
    "    # TODO: Entrena un modelo de regresión logística para data_train, target_train en ALTA dimensionalidad\n",
    "\n",
    "    # TODO: Determina las predicciones para x_test\n",
    "    pred =\n",
    "\n",
    "    # TODO: Calcula e imprime el accuracy para la predicción\n",
    "    score = ...\n",
    "    print(\"Accuracy: \", score)\n",
    "\n",
    "    # TODO: Reduce la dimensionalidad de los datos data_test usando PCA y TSNE respectivamente\n",
    "    if dim_red == 'TSNE':\n",
    "        reduced_test = ...\n",
    "    else:\n",
    "        reduced_test = ...\n",
    "    return pred, reduced_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda graficará usando tu implementación anterior, las predicciones correctas hechas por el modelo entrenado en alta dimensionalidad para el conjunto de prueba en verde y las predicciones incorrectas en rojo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(reduced_data, pred, target, ax, title):\n",
    "    c =  np.where(pred == target, 'g', 'r')\n",
    "    ax.scatter(\n",
    "        reduced_data[:, 0],\n",
    "        reduced_data[:, 1],\n",
    "        color = c \n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "# Train set\n",
    "X = data_train.to_numpy()\n",
    "y = target_train.to_numpy()\n",
    "\n",
    "# Test set\n",
    "X_test = data_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), constrained_layout=True)\n",
    "fig.suptitle(f\"Entrenamiento en alta dimensionalidad (correctas -> verde)\", y=1.05)\n",
    "############ Originales #############\n",
    "for c in range(2):\n",
    "    # =========  Visualizacion de predicciones ==== #\n",
    "    pred, reduced_x = train_pred_high_dim(X, y, X_test, y_test, 'PCA')\n",
    "    ax[0] = plot_preds(reduced_x, pred, y_test, ax[0], 'Dimensionalidad reducida con (PCA)')\n",
    "\n",
    "    # Entrenamiento baja dimensionalidad\n",
    "    pred, reduced_x = train_pred_high_dim(X, y, X_test, y_test, 'TSNE')\n",
    "    ax[1] = plot_preds(reduced_x, pred, y_test, ax[1], 'Dimensionalidad reducida con (TSNE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Entrenando en datos reducidos\n",
    "En las siguientes celdas probaremos analizar la calidad de la predicción si entrenamos en alta y en baja dimensionalidad. Completa la siguiente celda  para generar las predicciones del conjunto de entrenamiento al entrenar en alta y baja dimensionalidad respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ====================  BAJA dimensionalidad ====================#\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def train_pred_low_dim(x_train, y_train, x_test, y_test):\n",
    "    n = x_train.shape[0]\n",
    "    # TODO: Entrena un modelo DE PCA usando x_train para reducir la dimensionalidad a 2 dimensiones\n",
    "    pca = ...\n",
    "    \n",
    "    # TODO: Utiliza el modelo anterior para reducir la dimensionalidad de x_train a N, 2\n",
    "    reduced_train = ...\n",
    "\n",
    "    assert reduced_train.shape == (n, 2), f\"x_train_reduced debería ser {n,2} pero es {reduced_train.shape}\"\n",
    "\n",
    "    # TODO: Entrena un modelo de regresión logística para usando x_train_reduced\n",
    "\n",
    "    # TODO: Reduce la dimensionalidad de los datos data_test usando EL MISMO MODELO de PCA\n",
    "    # que el que usaste para reducir x_train. No lo entrenes de nuevo\n",
    "    reduced_test = ...\n",
    "\n",
    "    # TODO: Determina las predicciones para reduced_test\n",
    "    pred = ...\n",
    "\n",
    "    # TODO: Calcula e imprime el accuracy de la predicción\n",
    "    score = ...\n",
    "    print(f\"Accuracy: {score}\")\n",
    "    return pred, reduced_train, reduced_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes usar la siguiente celda para validar que lo que regresa el código anterior tenga las dimensionalidades esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "X = data_train.to_numpy()\n",
    "y = target_train.to_numpy()\n",
    "\n",
    "# Test set\n",
    "X_test = data_test.to_numpy()\n",
    "y_test = target_test.to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), constrained_layout=True)\n",
    "fig.suptitle(f\"Entrenamiento en BAJA dimensionalidad (correctas -> verde)\", y=1.05)\n",
    "############ Originales #############\n",
    "# =========  Visualizacion de predicciones ==== #\n",
    "# Entrenamiento baja dimensionalidad PCA\n",
    "pred, reduced_xtrain, reduced_xtest = train_pred_low_dim(X, y, X_test, y_test)\n",
    "for c in range(2):\n",
    "    # Clases reales\n",
    "    ax[0].scatter(\n",
    "        reduced_xtest[y_test == c][:, 0],\n",
    "        reduced_xtest[y_test == c][:, 1],\n",
    "    )\n",
    "    ax[0].set_title('Clases reales (Ground Truth)')\n",
    "\n",
    "ax[1] = plot_preds(reduced_xtest, pred, y_test, ax[1], 'Dimensionalidad reducida con (PCA)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En las imágenes mostramos:\n",
    "- En verde, los puntos clasificados correctamente\n",
    "- En rojo, los puntos clasificados incorrectamente\n",
    "\n",
    "Considerando las imágenes del entrenamiento de alta y baja dimensionalidad, responde:\n",
    "\n",
    "- Para el ejemplo de regresión logística y clasificación de tumores. ¿Cuál modelo tiene mejor accuracy?¿Funciona mejor entrenar en alta o baja dimensionalidad?\n",
    "- Investiga: ¿Es posible para T-SNE reducir la dimensionalidad de datos nuevos usando el mismo modelo que para los datos de entrenamiento?\n",
    "- ¿Por qué en el entrenamiento de baja dimensionalidad se omite TSNE?\n",
    "- ¿Es posible para PCA reducir la dimensionalidad de datos nuevos usando el mismo modelo que para los datos de entrenamiento?\n",
    "- ¿De que nos puede servir reducir la dimensionalidad de los datos?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
